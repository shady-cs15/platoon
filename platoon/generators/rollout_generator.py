from dataclasses import asdict
from typing import Callable
from pathlib import Path
import logging
from concurrent.futures import ProcessPoolExecutor, as_completed, TimeoutError
# import asyncio
import json
from typing import Any, Dict
from platoon.generators.types import RolloutGeneratorConfig
import traceback


class RolloutGenerator:
    def __init__(self, config: RolloutGeneratorConfig):
        self.config = config
        self.logger = self._setup_logging()
        Path(self.config.output_dir).mkdir(parents=True, exist_ok=True)

        # Ensure we use the "spawn" start-method for full process isolation to avoid
        # interactions between forked processes and asyncio event loops.
        import multiprocessing as mp
        try:
            mp.set_start_method("spawn")
            self.logger.info("Set start method to spawn in rollout generator.")
        except RuntimeError:
            # The start method can only be set once per interpreter session.
            self.logger.warning("Failed to set start method to spawn in rollout generator.")
            pass

        # Create a single process pool that is reused across batches.
        self.executor = ProcessPoolExecutor(max_workers=self.config.max_parallel_processes)

    def _setup_logging(self):
        logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO if self.config.verbose else logging.WARNING)
        return logger

    def generate_rollouts_batch(self, task_ids: list[str], rollout_fn: Callable[[str], dict]):
        """Generate rollouts for a batch of tasks using multiprocessing.
        
        Args:
            task_ids: List of task IDs to process
            rollout_fn: Function that takes a task ID and returns a dict representation of the TrajectoryCollection generated by the rollout.
            
        Returns:
            List of trajectory data dictionaries
        """
        # Convert config to dict for pickling
        config_dict = asdict(self.config)
        # loop = asyncio.get_event_loop()
        # tasks = []

        # for task_id in task_ids:
        #     args = (task_id, config_dict)
        #     future = loop.run_in_executor(self.executor, rollout_fn, args)
        #     tasks.append(future)

        # # Gather results; using `return_exceptions=True` ensures that failures in one
        # # subprocess don't cause the entire batch to hang indefinitely.
        # results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # for result in results:
        #     if isinstance(result, Exception):
        #         self.logger.error("Exception in rollout", exc_info=(type(result), result, result.__traceback__))
        
        # results = [result for result in results if not isinstance(result, Exception)]
        
        # return results
        
        # Prepare arguments for each process
        process_args = [(task_id, config_dict) for task_id in task_ids]
        
        # Use ProcessPoolExecutor for true parallelism
        # Maximum time (seconds) to wait for the entire batch to finish. Prevents hanging indefinitely.
        timeout_seconds = self.config.per_rollout_timeout_seconds

        # Submit all tasks to the shared executor
        future_to_task = {
            self.executor.submit(rollout_fn, args): args[0]
            for args in process_args
        }

        results = []
        completed = 0
        total = len(task_ids)

        # Collect results as they complete, but abort if we hit the timeout
        try:
            for future in as_completed(future_to_task, timeout=timeout_seconds):
                task_id = future_to_task[future]
                completed += 1
                try:
                    result = future.result()
                    results.append(result)

                    if self.config.verbose:
                        success = "error" not in result
                        status = "✅" if success else "❌"
                        self.logger.info(f"{status} [{completed}/{total}] Task {task_id} completed")

                except Exception as e:
                    error_message = f"❌ [{completed}/{total}] Task {task_id} failed with exception: {e}\n{traceback.format_exc()}"
                    self.logger.error(error_message)
                    results.append({
                        "task_id": task_id,
                        "error": str(e),
                        "success": False
                    })
        except TimeoutError:
            self.logger.error(f"Batch timed out after {timeout_seconds}s. Cancelling unfinished tasks and resetting executor.")
            for fut, t_id in future_to_task.items():
                if not fut.done():
                    fut.cancel()
                    results.append({
                        "task_id": t_id,
                        "error": "Timeout",
                        "success": False
                    })
            # Stop accepting new tasks and do not wait for running ones; they will self-timeout inside the child
            # try:
            #     self.executor.shutdown(wait=False, cancel_futures=True)
            # except Exception:
            #     pass
            # # Re-create a fresh executor for subsequent batches
            # self.executor = ProcessPoolExecutor(max_workers=self.config.max_parallel_processes)
            
        self.executor.shutdown(wait=False, cancel_futures=True)
        self.executor = ProcessPoolExecutor(max_workers=self.config.max_parallel_processes)

        results = [result for result in results if "error" not in result]

        return results

    def save_rollouts(self, trajectory_collection_dumps: list[Dict[str, Any]], batch_num: int = 0):
        """Save trajectory data to disk.
        
        Args:
            trajectory_collection_dumps: List of serialized trajectory collection data dictionaries
            batch_num: Batch number for filename
        """
        output_path = Path(self.config.output_dir)
        
        filepath = output_path / f"trajectories_batch_{batch_num}.jsonl"
        with open(filepath, 'w') as f:
            for traj_dump in trajectory_collection_dumps:
                if isinstance(traj_dump, dict) and "error" not in traj_dump:
                    f.write(json.dumps(traj_dump) + '\n')
    
        self.logger.info(f"Saved {len(trajectory_collection_dumps)} trajectory collections to {filepath}")

    def close(self):
        """Shut down the shared process pool. Safe to call multiple times."""
        if hasattr(self, "executor"):
            self.executor.shutdown(wait=True)
