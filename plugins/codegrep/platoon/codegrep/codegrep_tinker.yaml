# CodeGrep Tinker Training Configuration

# Training configuration
train:
  model_name: Qwen/Qwen3-4B
  renderer_name: qwen3
  batch_size: 32
  num_epochs: 10
  num_minibatches: 1
  num_microbatches: 1
  max_staleness: 3
  lora_rank: 32
  loss_fn: cispo
  loss_fn_config:
    clip_low_threshold: 0.0
    clip_high_threshold: 5.0
  optimizer:
    learning_rate: 1e-6
    beta1: 0.9
    beta2: 0.999
    eps: 1e-8
  workflow_config:
    group_size: 8
    rollout_config:
      max_steps: 30
      output_dir: ./rollout_results
      verbose: true
      timeout: 600
  num_concurrent_rollout_workflow_workers: 32

# Eval configuration
eval:
  strategy: epoch
  every: 1
  num_concurrent_rollout_workflow_workers: 50
  workflow_config:
    group_size: 1
    rollout_config:
      max_steps: 30
      output_dir: ./eval_results
      verbose: false
      timeout: 600

# Checkpoint configuration
checkpoint:
  strategy: epoch
  every: 5
  load_checkpoint_path: null

# Logging
log_path: ./logs
tinker_base_url: null

# Stats and WandB configuration
stats:
  experiment_name: codegrep-tinker
  trial_name: grpo-trial1
  wandb:
    mode: online
    project: codegrep-tinker
    entity: null
    tags:
      - codegrep
      - tinker
      - grpo

